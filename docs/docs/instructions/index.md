---
layout: docs
title: Instructions
nav_order: 2
has_children: true
---

# ğŸ› ï¸ Instructions

**FastFlowLM (FLM)** is a deeply optimized runtime for **local LLM inference on AMD NPUs** â€”  
ultra-fast, power-efficient, and 100% offline.

Its user interface and workflow are similar to **Ollama**, but purpose-built for AMD's XDNA architecture.

This section will walk you through how to use FastFlowLM with examples.

---

## ğŸ“š Sections

- [CLI basics](cli/)
- [Server basics](server/basics/)
- [OpenAPI / client usage](server/openapi/)
- [WebUI](server/webui/)
- [LangChain RAG](server/rag_LangChain/)
- [LangChain Web Search](server/websearch_LangChain/)
- [Obsidian integration](server/obsidian/)
- [Microsoft AI Toolkit](server/msft_AI_toolkit/)