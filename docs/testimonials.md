---
layout: page
title: "Testimonials"
permalink: /testimonials/
description: "What developers and partners are saying about FastFlowLM on NPU-first architectures."
---

<section class="section">
  <div class="section__inner">
    <header class="section-header">
      <p class="kicker">Testimonials</p>
      <h1 class="headline-md">What developers, partners, and silicon teams are saying</h1>
      <p class="copy-md">
        FastFlowLM combines deep knowledge of LLM architecture with world-class optimization for hardware-level parallelism. The result is the industry’s first—and only—runtime truly engineered for NPU-accelerated AI inference. Our debut product, deployed on AMD’s Ryzen™ AI platform, is revolutionizing on-device intelligence with unprecedented speed, efficiency, and responsiveness.
        These stories highlight how an NPU-first runtime lands in the real world—from beta cohorts and AMD’s own AI engineering
        org to third-party benchmarking labs.
      </p>
    </header>

    <div class="card-grid card-grid--testimonials">
      <article class="card testimonial-card">
        <div class="testimonial-card__meta">
          <span class="card__label">User Adoption</span>
          <h3 class="testimonial-card__title">Overwhelming user trials and responses</h3>
        </div>
        <blockquote class="testimonial-card__quote">
          “Within hours of the beta release, thousands of builders pulled FastFlowLM from GitHub and ran it on their own Ryzen™ AI laptops.”
        </blockquote>
        <ul class="testimonial-card__bullets">
          <li><strong>Within hours of the beta release:</strong> Thousands of users pulled FastFlowLM from GitHub and ran it on their own hardware.</li>
          <li><strong>Community content:</strong> Early users independently produced <a href="https://www.youtube.com/watch?v=oyTo_D7aw60">videos and walkthroughs</a> showing AMD NPUs are <em>far from useless</em> with the right runtime.</li>
          <li><strong>Developer competitions:</strong> The winning team in a global AI PC developer contest chose FastFlowLM as their NPU runtime.</li>
          <li><strong>Customer feedback:</strong> One early customer wrote that our solution “seems to be the most elegant so far for AMD NPUs.”</li>
        </ul>
        <div class="testimonial-card__source">
          <span class="testimonial-card__name">FastFlowLM beta cohort</span>
          <span class="testimonial-card__role">First 72 hours post-launch</span>
        </div>
      </article>

      <article class="card testimonial-card">
        <div class="testimonial-card__meta">
          <span class="card__label">AMD AI Team</span>
          <h3 class="testimonial-card__title">Feedback from AMD AI engineering leaders</h3>
        </div>
        <blockquote class="testimonial-card__quote">
          “We’re interested in FLM. I spent considerable effort in getting Copilot up on our AIE/NPU and it is a difficult beast. Your kernels and model implementations appear to be closed source but your perf numbers seem impressive.”
        </blockquote>
        <ul class="testimonial-card__bullets">
          <li><strong>Kernel fidelity:</strong> Tile-optimized operators map directly to AMD’s AIE architecture.</li>
          <li><strong>Model coverage:</strong> Flagship reasoning, multimodal, and MoE models stay inside Ryzen™ AI silicon limits.</li>
          <li><strong>Confidence to ship:</strong> AMD’s own field teams reference FastFlowLM in partner enablement sessions.</li>
        </ul>
        <div class="testimonial-card__source">
          <span class="testimonial-card__name">Senior AMD AI team leaders</span>
          <span class="testimonial-card__role">Ryzen™ AI Architecture (AIE/NPU)</span>
        </div>
      </article>

      <article class="card testimonial-card">
        <div class="testimonial-card__meta">
          <span class="card__label">Performance Engineering</span>
          <h3 class="testimonial-card__title">Proof from independent benchmarking labs</h3>
        </div>
        <blockquote class="testimonial-card__quote">
          “Real-time NPU inference is not just possible, but practical for everyday users.”
        </blockquote>
        <blockquote class="testimonial-card__quote">
          “Gaming? No time for that. How about running Llama3.1 8B on the AMD Ryzen AI Z2 Extreme NPU in the ROG Xbox Ally X Via FastFlowLM instead?" ”
        </blockquote>        
        <ul class="testimonial-card__bullets">
          <li><strong>Llama 3.2:3B:</strong> Demonstrated on an AMD Ryzen™ AI device with steady token streaming.</li>
          <li><strong>Thermal headroom:</strong> Runs stay under 2W, extending battery life versus GPU-bound stacks.</li>
          <li><strong>Agent workflows:</strong> Deterministic latency keeps step-by-step chains responsive in demos.</li>
        </ul>
        <div class="testimonial-card__source">
          <span class="testimonial-card__name">Client performance director</span>
          <span class="testimonial-card__role">Global benchmarking firm</span>
        </div>
      </article>
    </div>

    <div class="section-block section-block--panel testimonial-cta">
      <p class="headline-sm">Share your FastFlowLM story</p>
      <p class="copy-md">We’re continuing to collect stories from developers, OEM partners, and researchers running FastFlowLM on real Ryzen™ AI hardware.</p>
      <ul>
        <li><strong>Developer workflows:</strong> How FastFlowLM fits into your local dev loop, CI, or production agents.</li>
        <li><strong>NPU performance wins:</strong> Concrete improvements in latency, throughput, or power draw vs. GPU-first stacks.</li>
        <li><strong>Use cases:</strong> From local assistants and multimodal copilots to privacy-preserving RAG and on-device analytics.</li>
      </ul>
      <p class="copy-md">
        If you’d like to be featured here, reach out at <a href="mailto:info@fastflowlm.com">info@fastflowlm.com</a> or in the FastFlowLM Discord.
      </p>
    </div>
  </div>
</section>

