<section class="section" id="models">
  <div class="section__inner">
    <div class="section-header">
      <p class="kicker">Models</p>
      <h2 class="headline-md">One CLI, every Ryzen-ready model</h2>
      <p class="copy-md">
        Pull curated FastFlowLM recipes.
        The runtime streams tokens via HTTP, WebSocket, or the Ollama-compatible API, so existing apps work without
        rewrites.
      </p>
    </div>
    <div class="card-grid">
      <article class="card">
        <p class="card__label">Flagship reasoning</p>
        <h3>Llama 3.2 路 DeepSeek 路 Qwen 3</h3>
        <p>Optimized kernels for 70B down to 1B, with automatic quantization and smart context reuse.</p>
      </article>
      <article class="card">
        <p class="card__label">Vision &amp; speech</p>
        <h3>Gemma 3 VLM 路 Whisper 路 Gemma Audio</h3>
        <p>VLM and audio pipelines render directly on the NPU, enabling private multimodal assistants.</p>
      </article>
      <article class="card">
        <p class="card__label">Edge fine-tuning</p>
        <h3>FLM MoE + Embedding suites</h3>
        <p>Use built-in adapters, LoRA checkpoints, and embedding endpoints for retrieval workflows.</p>
      </article>
    </div>
    <div class="btn-row">
      <a class="btn btn--ghost" href="{{ '/models/' | relative_url }}">Browse models</a>
      <a class="btn btn--ghost" href="{{ '/docs/models/' | relative_url }}">Model docs</a>
    </div>
  </div>
</section>